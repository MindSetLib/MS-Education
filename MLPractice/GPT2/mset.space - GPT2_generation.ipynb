{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation using GPT2. Opentalks.ai workshop notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://mset.space - ml platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.2.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from transformers) (20.8)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.55.2)\n",
      "Collecting tokenizers==0.9.4\n",
      "  Downloading tokenizers-0.9.4-cp38-cp38-manylinux2010_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 13.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.0)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893258 sha256=2b52c4c46310e604e7c96931454c281c4fd76e95c65e6fa4fe2f8f857a382c9a\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/7b/78/f4/27d43a65043e1b75dbddaa421b573eddc67e712be4b1c80677\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083a3840290747b5ad4686a37a532c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a017ae9b7a974ed0a89285bb46aecda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5987d3a167450b92a312f9e0160087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552fed0c488e491bbf8bfd6d8555b7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights_shortcut = 'gpt2'\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(weights_shortcut)\n",
    "model = GPT2LMHeadModel.from_pretrained(weights_shortcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = 'My name is Bob, I am did project with claims department'\n",
    "encoded_prompt = tokenizer.encode(prompt_text, return_tensors=\"pt\")\n",
    "bad_words_ids = [tokenizer.encode(bad_word, add_prefix_space=True) for bad_word in ['idiot', 'stupid', 'shut up']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3666, 1438,  318, 5811,   11,  314,  716,  750, 1628,  351, 3667, 5011]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "encoded_prompt = encoded_prompt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frank lost his keys -> ~~~\n",
      "\n",
      "The next time I was in the room, I saw a man with a big black hat. He was wearing a white hat and a black tie. He said, \"I'm going to take you\n"
     ]
    }
   ],
   "source": [
    "encoded_result = model.generate(encoded_prompt, \n",
    "                                eos_token_id=tokenizer.eos_token_id, max_length= 50,\n",
    "                                temperature=5, top_p=0.1, repetition_penalty=1.01,bad_words_ids=bad_words_ids\n",
    "                               )\n",
    "result = tokenizer.decode(encoded_result[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([17439,  2626,   465,  8251,  4613,   220,  4907,    93,   198,   198,\n",
       "          464,  1306,   640,   314,   373,   287,   262,  2119,    11,   314,\n",
       "         2497,   257,   582,   351,   257,  1263,  2042,  6877,    13,   679,\n",
       "          373,  5762,   257,  2330,  6877,   290,   257,  2042,  9839,    13,\n",
       "          679,   531,    11,   366,    40,  1101,  1016,   284,  1011,   345],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is preprocessed from here: https://github.com/square/MimicAndRephrase/tree/master/datasets/Sentiment/Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_dataset_tensor(dataset_path):\n",
    "    with open(dataset_path) as f:\n",
    "        tokenized_dataset = [tokenizer.encode(line) for line in f]\n",
    "\n",
    "    samples_num = len(tokenized_dataset)\n",
    "    max_tokens_num = max(map(len, tokenized_dataset))\n",
    "\n",
    "    input_ids = np.full((samples_num, max_tokens_num), tokenizer.pad_token_id, dtype=np.int64)\n",
    "    for i, tokens in enumerate(tokenized_dataset):\n",
    "        input_ids[i, :len(tokens)] = tokens\n",
    "\n",
    "    return torch.from_numpy(input_ids)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_data_tensor = get_dataset_tensor(dataset_path='paraphrase_dataset.txt')\n",
    "train_dataloader = DataLoader(train_data_tensor, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def train_model(model, training_data, epochs_num):\n",
    "    optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=1)\n",
    "\n",
    "    train_loss = []\n",
    "\n",
    "    for _ in tqdm(range(epochs_num), total=epochs_num):\n",
    "        for input_ids in training_data:\n",
    "            model.train()\n",
    "\n",
    "            input_ids = input_ids.to(device)\n",
    "            loss = model(input_ids, labels=input_ids)[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "                \n",
    "    return model, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([17439,  2626,   465,  8251,  4613,   220,  4907,    93,   198,   198,\n",
       "          464,  1306,   640,   314,   373,   287,   262,  2119,    11,   314,\n",
       "         2497,   257,   582,   351,   257,  1263,  2042,  6877,    13,   679,\n",
       "          373,  5762,   257,  2330,  6877,   290,   257,  2042,  9839,    13,\n",
       "          679,   531,    11,   366,    40,  1101,  1016,   284,  1011,   345],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frank lost his keys -> ix's is the one for me :D. Also, and for\n"
     ]
    }
   ],
   "source": [
    "encoded_prompt = tokenizer.encode('Frank lost his keys -> ', return_tensors=\"pt\").to(device)\n",
    "encoded_result = model.generate(encoded_prompt, \n",
    "                                eos_token_id=tokenizer.eos_token_id, do_sample=True)\n",
    "result = tokenizer.decode(encoded_result[0], skip_special_tokens=True)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  464,  9294,   286,  8237,   373,   269,   967,   276,  4613,   314,\n",
       "          716,  6507,   546,   262,  8237,     0,   198, 50256, 50256, 50256,\n",
       "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "        50256, 50256])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-608cb9de89ed>:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for _ in tqdm(range(epochs_num), total=epochs_num):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bbd124761fa4ed4965c7c4d743c4ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finetuned_model, metrics_history = train_model(model, train_dataloader, epochs_num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "encoded_prompt = tokenizer.encode('I have a car acident today -> ', return_tensors=\"pt\").to(device)\n",
    "encoded_result = finetuned_model.generate(encoded_prompt, \n",
    "                                          eos_token_id=tokenizer.eos_token_id,\n",
    "                                          num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a car acident today -> ive had a car accident and it's totaled\u0001 and\n"
     ]
    }
   ],
   "source": [
    "result = tokenizer.decode(encoded_result[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   40,   423,   257,  1097,   936,   738,  1909,  4613,   220,   425,\n",
       "          550,   257,  1097,  5778,   290,   340,   338, 39398,   189,   290],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      " have\n",
      " a\n",
      " car\n",
      " ac\n",
      "ident\n",
      " today\n",
      " ->\n",
      " \n",
      "ive\n",
      " had\n",
      " a\n",
      " car\n",
      " accident\n",
      " and\n",
      " it\n",
      "'s\n",
      " totaled\n",
      "\u0001\n",
      " and\n"
     ]
    }
   ],
   "source": [
    "for cur_sample_tokens in encoded_result[0]:\n",
    "    # print(int(cur_sample_tokens))\n",
    "    print(tokenizer.decode(int(cur_sample_tokens), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "* Compute validation metrics: perplexity/BLEU/ROUGE\n",
    "* Logging into tensorboard\n",
    "* Generate N candidates and filter or rerank\n",
    "* Analyze errors and improve dataset\n",
    "* Improve training: masking, `lr_scheduler`, multi-gpu training\n",
    "* Improve generation: try different strategies\n",
    "* Improve the model: use bigger model, try different architecrures (DialoGPT2, XLNet, CTRL  etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
