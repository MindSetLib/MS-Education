{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HistoryIC.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPQEqd/47lMckse+a80hiDc"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KNI5JB2mLGtm"},"source":["![](https://repository-images.githubusercontent.com/189893828/719c1380-8615-11e9-97b3-19bc06b0350a)"]},{"cell_type":"markdown","metadata":{"id":"yqiK69VRkwP4"},"source":["### Ранжирование"]},{"cell_type":"markdown","metadata":{"id":"2fcNd_rOZXXf"},"source":["Один из первых не нейросетевых подходов был предложен командой [Farhadi et al.](https://homes.cs.washington.edu/~ali/papers/sentence.pdf) в 2010 году. Они построили процедуру оценки схожести изображения и описания. Эта модель использует промежуточное пространство в формате <объект, действие, сцена>, которое несёт смысл предложения/изображения. Изображения и описания отображаются в промежуточное пространство и оценивают схожесть между собой. Для обучения этой модели был создан датасет PASCAL 2008 images, в котором собраны триплеты <объект, действие, сцена> и к ним изображения и описания."]},{"cell_type":"markdown","metadata":{"id":"RG1GZsx9Z912"},"source":["![](https://s8.hostingkartinok.com/uploads/images/2021/01/87a87da240b2f4dd7e8e6f6332e86533.png)"]},{"cell_type":"markdown","metadata":{"id":"R_ugrINxkymg"},"source":["### Графовые модели генерации текста"]},{"cell_type":"markdown","metadata":{"id":"1DahCqHXV5zd"},"source":["Команда [Kulkarni  et al.](http://acberg.com/papers/baby_talk.pdf) в 2013 году впервые использовали алгоритмы компьютерного зрения в связке с графовой системой для генерации описания к изображению.\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"H2RQ5deYqtRR"},"source":["![](https://s8.hostingkartinok.com/uploads/images/2021/01/2b1953a31b261f8b867e5d1a5f8525a0.png)"]},{"cell_type":"markdown","metadata":{"id":"7oepBFUmrkKL"},"source":["1.   Детектируем объекты на изображении.\r\n","2.   Каждому детектированному объекту присваиваем атрибуты, отражающие свойства объекта.\r\n","3.   Каждая пара детектированных объектов сравнивается с помощью специальных функций, которые описывают взаимоотношение двух объектов.\r\n","4.   Строим [CRF](https://towardsdatascience.com/conditional-random-fields-explained-e5b8256da776) модель в которой узлами являются детектированные объекты, атрибуты и взаимоотношение объектов.\r\n","5.   Используя построенную модель CRF, предсказываем последовательность из объектов, атрибутов и взаимоотношений двух объектов.\r\n","6.   Используем полученную последовательность для генерации предложения используя готовые шаблоны.\r\n","\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"kcjtcqVtd0gz"},"source":["### Нейросетевые подходы\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"ztVe4-VRl5Jz"},"source":["#### CNN + RNN"]},{"cell_type":"markdown","metadata":{"id":"7byEH8KlsH8e"},"source":["В 2015 году [Oriol Vinyals  et al.](https://arxiv.org/abs/1411.4555) предложил Neural Image Caption (NIC) модель, которая объединяет в себе CNN и RNN архитектуры.\r\n","\r\n","Идея состоит в том, чтобы использовать архитектуру CNN как image encoder, который будет кодировать изображение в вектор фиксированной длины. Для этого в классической CNN для классификации изображений убирается последний полносвязный слой."]},{"cell_type":"markdown","metadata":{"id":"1UUoQSq2n-Sg"},"source":["![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/03/ya_explain.png)"]},{"cell_type":"markdown","metadata":{"id":"CbIyQieAobb-"},"source":["В качестве модели для генерации текста, используется LSTM. В качестве скрытого состояния модели, используется выход из CNN. Автором было предложено генерировать текст двумя способами:\r\n","\r\n","1. На шаге t, подаём выход LSTM на шаге t-1.\r\n","2. BeamSearch - метод, когда выбираем k самых вероятных токена из распределения слов в словаре. Генерируем возможные цепочки последовательностей текста, а потом выбираем самую вероятную из них."]},{"cell_type":"markdown","metadata":{"id":"uLnNJZcT0iYu"},"source":["![](https://3.bp.blogspot.com/-8P950NVeWIM/VzyfKG91BpI/AAAAAAAABAc/koVPi5wZvpQsdvAqHTPEQe2B-29PqZDOwCLcB/s1600/image04.png)"]},{"cell_type":"markdown","metadata":{"id":"wQYDBzPFwo9C"},"source":["В данной архитектуре есть два недостатка, с которым борятся до сих пор, предлагая различные решения. Этот недостаток заключается в том, что модель видит объекты, но не видит связи между ними. Например, видит человека, видит мяч в его руке, но не видит связи между объектов, не видит, что мяч в руках и с ним возможно происходит какое-то действие.\r\n","\r\n","Вторая проблема известна как \"семантический разрыв\", когда объект связан с каким-то асбтрактным понятием, модель эту связь не видит. Например, модель не понимает связь, что люди одеты в тёплую одежду, потому что на улице мороз (мороз в данном случае абстрактное понятие, которое модель не видит). \r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"jkxq9VKAyWJW"},"source":["Поэтом у дальнейшем разрабатывались методы, как можно внедрить информацию о взаимосвязи объектов изображенных на картинке."]},{"cell_type":"markdown","metadata":{"id":"RI3fsltSl3NU"},"source":["#### CNN + RNN + Attention"]},{"cell_type":"markdown","metadata":{"id":"gQW2N3y1mEiS"},"source":["Дальнейшее улучшение качества формирования описания к изображению заключалось в интегрировании механизма внимания, первые исследования были произведены [Xu et al. (2015)](https://arxiv.org/pdf/1502.03044v1.pdf), [Jin et al. (2016)](https://arxiv.org/pdf/1603.03925.pdf), [](). Т.к. RNN имеет последовательный вход, мы можем на каждом шаге генерирования описания, с помощью механизма внимания, посещать различные области изображения. Недостатком данного подхода является то, что модель посещает объекты на изображении, но не видит действия между ними, поэтому и внимание на это обратить не может."]},{"cell_type":"markdown","metadata":{"id":"hWMOdgZ0mFqy"},"source":["В период с 2015 по 2018, до выхода архитектуры BERT, для решения задачи image captioning активно исследовались различные модификации архитектуры NIC, новые методы тренировки, новые метрики качества [SPACE](http://search.arxiv.org:8081/paper.jsp?r=1607.08822&qid=1610984588578ler_nCnN_1655550553&qs=image+caption) и [CIDEr](https://arxiv.org/abs/1411.5726). Новые метрики качества были необходимы, т.к. существующие BLUE и ROUGE, слабо коррелировали с человеческим понятием качественного описания.\r\n","\r\n","Были удачные попытки применить метод обучения Reinforcement Learning (обучение с подкреплением). В частности [Liu et al. (2016)](https://arxiv.org/abs/1612.00370) предложили policy gradient (PG)\r\n","method для оптимизации линейной комбинации метрик SPACE и CIDEr. Так же [Ranzato et al. (2015)](https://arxiv.org/abs/1511.06732) и [Rennie et al. (2016)](https://arxiv.org/abs/1511.06732) проводили исследование в этом направлении."]},{"cell_type":"markdown","metadata":{"id":"E8HYP1ovwjqq"},"source":["Разрабатывали модели в которые внедряли семантические графы. [Yao et al. (2018)](https://openaccess.thecvf.com/content_ECCV_2018/papers/Ting_Yao_Exploring_Visual_Relationship_ECCV_2018_paper.pdf) GCN-LSTM модель, которая использует семантический граф."]},{"cell_type":"markdown","metadata":{"id":"lCpn0xSF4t4V"},"source":["![](https://s8.hostingkartinok.com/uploads/images/2021/01/143d87f079b5bcd1fa7c358fd630062f.jpg)"]},{"cell_type":"markdown","metadata":{"id":"ZWGbelO35Cc4"},"source":["Данная модель использует вначале R-CNN, задача которой детектировать определенные объекты на изображении. Далее строится два графа:\r\n","\r\n","1. Семантический граф, в котором вершинами являются детектированные объекты, а ребрами семантические связи между объектами.\r\n","\r\n","2. Пространственный граф, в котором вершинами являются детектированные объекты, а ребрами пространственные связи между объектами.\r\n","\r\n","Graph Convolutional Networks (GCN) используется, чтобы кодировать контекстуальную информацию (построенные графы) и обогатить ею информацию о детектированных объектах. \r\n","\r\n","Далее данные отправляются в LSTM модель с механизмом внимания.\r\n","\r\n","Во всех классических NIC моделях есть недостаток, заключенный в том, что CNN модель детектирует только объекты, но не связи между ними. Предложенная архитектура восполняет этот недостаток, но как пишут авторы, данный механизм ещё до конца не изучен. \r\n"]},{"cell_type":"markdown","metadata":{"id":"Po7ZUan4RQ0A"},"source":["### Transformer"]},{"cell_type":"markdown","metadata":{"id":"SEnI0CTVRTfm"},"source":["В 2017 году была представлена архитектура [Transformer](https://arxiv.org/abs/1706.03762) для языкового моделирования. Следующим этапом развития стала модернизация архитектуры Transformer для задачи image captioning. Ниже представлена архитектура Transformer для языкового моделирования."]},{"cell_type":"markdown","metadata":{"id":"TBd3WNnMprxu"},"source":["![](https://s8.hostingkartinok.com/uploads/images/2021/01/19ff9293c8153edf6a74cbfaf80d2128.jpg)"]},{"cell_type":"markdown","metadata":{"id":"K1eIiwrBo7Ju"},"source":["[Zhu et al. (2018)](https://www.researchgate.net/publication/325016817_Captioning_Transformer_with_Stacked_Attention_Modules) модифицировали архитектуру, заменив стандартный энкодер на CNN, подставляя модели Resnet 101, Faster-RCNN и др."]},{"cell_type":"markdown","metadata":{"id":"3Q_EDZNhtLCA"},"source":["![](https://s8.hostingkartinok.com/uploads/images/2021/01/8fbfbf825891c90809cfc9aec3b22a31.jpg)"]},{"cell_type":"markdown","metadata":{"id":"MbZGLxUWuY5m"},"source":["[Zhang et al. (2018)](https://easychair.org/publications/preprint_open/LPLX) добавили к энкодеру CNN архитектуру."]},{"cell_type":"markdown","metadata":{"id":"z57vvm20uZC7"},"source":["![](https://s8.hostingkartinok.com/uploads/images/2021/01/a213ceceea001288e8abb87506180431.jpg)"]},{"cell_type":"markdown","metadata":{"id":"g3ty3Zeqo2MS"},"source":["[Herdade et al. (2019)](https://arxiv.org/abs/1906.05963) предложили модификацию Transformer, внедрив в архитектуру object relation модуль, который использует информацию о пространственном расположении объектов относительно друг друга."]},{"cell_type":"markdown","metadata":{"id":"AKnzEb5LksCA"},"source":["![](https://s8.hostingkartinok.com/uploads/images/2021/01/9d078cfd7fa573df224b7f62f02e40fc.jpg)"]},{"cell_type":"markdown","metadata":{"id":"Awt4DKVhkw0F"},"source":["На изображении выше, зеленой рамкой выделены те объекты, которые связанны с объектом выделенным красной рамкой, а яркость показывает насколько важен объект для объекта выделенного красной рамкой.\r\n","\r\n","Схема модели изображена ниже."]},{"cell_type":"markdown","metadata":{"id":"P0GYGsR5lrab"},"source":["![](https://s8.hostingkartinok.com/uploads/images/2021/01/20572f1d2764860ce2796c0671e94166.jpg)"]},{"cell_type":"markdown","metadata":{"id":"YkCV5qmiefqy"},"source":["Намного более сложную архитектуру предложили [Zhu et al. (2019)](https://www.researchgate.net/publication/339558555_Entangled_Transformer_for_Image_Captioning) назвав её ETA-Transformer. \r\n","\r\n","Их модель включает в себя:\r\n","\r\n","1. EnTangled Attention (ETA) - механизм внимания, который позволяет одновременно использовать визуальную и семантическую информацию.\r\n","\r\n","2.  Gated Bilateral Controller (GBC) - механизм обеспечивающий управление прямого распространения сложной мультимодальной информации, а так же их градиентами обратного распространения ошибки."]},{"cell_type":"markdown","metadata":{"id":"-Peo0rV3eeUk"},"source":["![](https://s8.hostingkartinok.com/uploads/images/2021/01/a4637c0184eabe2aa9f5c7c55a50fc31.jpg)"]},{"cell_type":"markdown","metadata":{"id":"ZZ_KRrDAzeeL"},"source":["### SOTA"]},{"cell_type":"markdown","metadata":{"id":"2UuRCHFQyyU9"},"source":["Одна из лучших моделей на сегодняшний день для задачи image captioning, является архитектура **Meshed-Memory Transformer** или коротко **($M^2$ Transformer)**. \r\n","\r\n","Данная архитектура хороша тем, что для неё есть готовая модель на [GitHub](https://github.com/aimagelab/meshed-memory-transformer), подробная [научная статья](https://arxiv.org/pdf/1912.08226.pdf) и статья c пояснениями на [medium](https://medium.com/@silkworm/m2-meshed-memory-transformer-for-image-captioning-8a42515678ef)."]},{"cell_type":"markdown","metadata":{"id":"gH72pJD219_6"},"source":["![](https://s8.hostingkartinok.com/uploads/images/2021/01/655f1fdf854571f99bcf3b28530da499.jpg)"]},{"cell_type":"markdown","metadata":{"id":"B55mFFhyz13k"},"source":["Стандартная архитектура Transformer по своему построению не способна хранить знание о взаимоотношении между объектами. Например, если на изображении будет собака и кошка, то модель не знает о таком абстрактном понятии как погоня. Чтобы решить данную проблему, авторы встраивают в энкодер \"память\", которая является собой обучаемой матрицей, в которой хранятся априорные знания о взаимосвязи объектов и которая соединена с key и value.\r\n","\r\n","И второе нововведение, это перекрестное соединение энкодера и декодера. Сделано это для того, чтобы декодер мог использовать низкоуровневые и высокоуровневые фичи из энкодера, который так же кодирует детектированные объекты из изображения. Декодер при генерировании нового слова, использует выходы со всех энкодер слоёв."]},{"cell_type":"markdown","metadata":{"id":"3DC2XhyaQmNY"},"source":["### Датасеты для image captioning"]},{"cell_type":"markdown","metadata":{"id":"HLGbM0LlQprN"},"source":["[MSCOCO](https://cocodataset.org/#download) (содержит 180k изображений)\r\n","\r\n","Flickr 8k (содержит 8k изображений)\r\n","\r\n","Flickr 30k (содержит 30k изображений)\r\n","\r\n","[Conceptual Captions,](https://github.com/google-research-datasets/conceptual-captions) содержит ~3.3 мил. изображений, [статья](https://ai.googleblog.com/2018/09/conceptual-captions-new-dataset-and.html).\r\n","\r\n","[TextCaps](https://arxiv.org/pdf/2003.12462.pdf) (содержит 28k изображений), [статья](https://arxiv.org/pdf/2003.12462.pdf). Ключевая идея в том, чтобы модель могла считывать текст имеющийся на изображении.\r\n","\r\n","[Im2Text](http://www.cs.virginia.edu/~vicente/sbucaptions/) \r\n","\r\n","[nocaps](https://nocaps.org/) (содержит 15k изображений), [статья](https://arxiv.org/abs/1812.08658).\r\n"]},{"cell_type":"markdown","metadata":{"id":"PZuGL21EQ4vE"},"source":["### Дополнительные материалы"]},{"cell_type":"markdown","metadata":{"id":"N5igT-YdJ65Q"},"source":["### CRF"]},{"cell_type":"markdown","metadata":{"id":"N40sdF6cQ7Wi"},"source":["Простое объяснение и реализация CRF на примере игральных костей: https://towardsdatascience.com/conditional-random-field-tutorial-in-pytorch-ca0d04499463\r\n","\r\n","Применение CRF в CV: https://towardsdatascience.com/conditional-random-fields-explained-e5b8256da776\r\n","\r\n","Полный туториал по CRF: https://towardsdatascience.com/implementing-a-linear-chain-conditional-random-field-crf-in-pytorch-16b0b9c4b4ea"]},{"cell_type":"markdown","metadata":{"id":"1572leBoKbO_"},"source":["### Transformer based"]},{"cell_type":"markdown","metadata":{"id":"K0dVDOadvI3u"},"source":["Transformers baseline: https://github.com/krasserm/fairseq-image-captioning\r\n","\r\n","M2 Transformer article: https://arxiv.org/pdf/1912.08226.pdf\r\n","\r\n","M2 Transformer github: https://github.com/aimagelab/meshed-memory-transformer \r\n","\r\n","M2 Transformer medium: https://medium.com/@silkworm/m2-meshed-memory-transformer-for-image-captioning-8a42515678ef\r\n","\r\n","Transformer 20.10.2020: https://arxiv.org/pdf/2004.14231.pdf \r\n","\r\n","ETA-Transformer: http://ffmpbgrnn.github.io/publications/pdf/ETA.pdf\r\n","\r\n","Boosted Transformer: https://www.mdpi.com/2076-3417/9/16/3260/htm\r\n"]},{"cell_type":"markdown","metadata":{"id":"LBs-NpcJJVng"},"source":["### Метрика"]},{"cell_type":"markdown","metadata":{"id":"LZeaw4ETJXn1"},"source":["Метрика для image captioning (2020): http://search.arxiv.org:8081/paper.jsp?r=2012.13137&qid=1610983120596ler_nCnN_1655550553&qs=image+caption&byDate=1\r\n","\r\n","SPICE метрика (2016): http://search.arxiv.org:8081/paper.jsp?r=1607.08822&qid=1610984588578ler_nCnN_1655550553&qs=image+caption"]},{"cell_type":"markdown","metadata":{"id":"kPWRGCgDLJSc"},"source":["### Advance"]},{"cell_type":"markdown","metadata":{"id":"cYhq7yZCLNXQ"},"source":["Быстрая генерация описаний (2019): http://search.arxiv.org:8081/paper.jsp?r=1912.06365&qid=1610984588578ler_nCnN_1655550553&qs=image+caption\r\n","\r\n","CNN отображает объекты на фотографии, но не связь между ними, решение (2019): http://search.arxiv.org:8081/paper.jsp?r=1912.01881&qid=1610984588578ler_nCnN_1655550553&qs=image+caption\r\n","\r\n","NIC куда вложить информацию о изображении (2017): http://search.arxiv.org:8081/paper.jsp?r=1703.09137&qid=1610984588578ler_nCnN_1655550553&qs=image+caption\r\n","\r\n","NIC генерирует смешные описания (2018): http://search.arxiv.org:8081/paper.jsp?r=1805.11850&qid=1610984588578ler_nCnN_1655550553&qs=image+caption\r\n","\r\n","Если встроить модель для кодирования вектора описания, то сеть будет лучше? (2019): http://search.arxiv.org:8081/paper.jsp?r=1901.01216&qid=1610984588578ler_nCnN_1655550553&qs=image+caption\r\n","\r\n","Объединение визуальной и семантической информации в механизме внимания (2019): http://search.arxiv.org:8081/paper.jsp?r=1909.02489&qid=1610984588578ler_nCnN_1655550553&qs=image+caption\r\n","\r\n","Встраивание предварительных знаний в NIC (2020): http://search.arxiv.org:8081/paper.jsp?r=1911.10082&qid=1610984588578ler_nCnN_1655550553&qs=image+caption\r\n","\r\n","NIC улучшение механизма внимания AAT: http://search.arxiv.org:8081/paper.jsp?r=1909.09060&qid=1610976663537ler_nCnN_1353746279&qs=image+caption "]},{"cell_type":"markdown","metadata":{"id":"OtQyqcqljAyn"},"source":["### Методы тренировки"]},{"cell_type":"markdown","metadata":{"id":"INYahuurjCut"},"source":["VIVO способ тренировки Transformer (2020): http://search.arxiv.org:8081/paper.jsp?r=2009.13682&qid=1610984588578ler_nCnN_1655550553&qs=image+caption  "]},{"cell_type":"markdown","metadata":{"id":"WLqm2kZYPVRB"},"source":["### Смежные темы"]},{"cell_type":"markdown","metadata":{"id":"xgHe6ZtyPXBI"},"source":["VQA задаем модели вопрос, а она ищет ответ в изображении: http://search.arxiv.org:8081/paper.jsp?r=1605.01379&qid=1610984588578ler_nCnN_1655550553&qs=image+caption\r\n","\r\n","Дистиляция и парсинг данных с Google Conceptual Captions: http://search.arxiv.org:8081/paper.jsp?r=2012.11691&qid=1610984588578ler_nCnN_1655550553&qs=image+caption\r\n","\r\n","A Guide to Image Caption: https://towardsdatascience.com/a-guide-to-image-captioning-e9fd5517f350"]}]}